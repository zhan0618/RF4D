<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RF4D:Neural Radar Fields for Novel View Synthesis in
Outdoor Dynamic Scenes</title>
  <link rel="icon" type="image/x-icon" href="static/images/ico.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">RF4D:Neural Radar Fields for Novel View Synthesis in
Outdoor Dynamic Scenes</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=K3AWi04AAAAJ&hl=zh-CN&oi=ao" target="_blank">Jiarui Zhang</a>,</span>
                <span class="author-block">
                  <a href="https://lizhihao6.github.io" target="_blank">Zhihao Li</a>,</span>
                  <span class="author-block">
                     <a href="https://scholar.google.com/citations?user=QlZK_hQAAAAJ&hl=zh-CN&oi=ao" target="_blank">Chong Wang</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=ypkClpwAAAAJ&hl=zh-CN" target="_blank">Bihan Wen</a><sup>*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zhan0618/RF4D_code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.20967" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
  
      <div class="columns is-centered">
      <div class="column is-full-width">
          <h2 class="title is-3">Motivation</h2>
          <img src="static/images/fig1.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>

          <div class="content has-text-justified">
            <p>
               (a) Comparison of radar view synthesis results for a dynamic scene with a moving vehicle (red box). Our method successfully renders the moving object, while Radar Fields fails to recover it. (b) Predicted occupancy and reflectance (Radar Fields) versus occupancy and RCS (ours). Our predictions follow radar physics, where high occupancy corresponds to strong RCS, while Radar Fields fails to maintain such consistency between occupancy and reflectance.
              </p>
          </div>
      </div>
      </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Neural fields (NFs) have demonstrated remarkable performance in scene reconstruction, powering various tasks such as novel view synthesis. However, existing NF methods relying on RGB or LiDAR inputs often exhibit severe fragility to adverse weather, particularly when applied in outdoor scenarios like autonomous driving. In contrast, millimeter-wave radar is inherently robust to environmental changes,  while unfortunately, its integration with NFs remains largely underexplored. Besides, as outdoor driving scenarios frequently involve moving objects, making spatiotemporal modeling essential for temporally consistent novel view synthesis. To this end, we introduce RF4D, a radar-based neural field framework specifically designed for novel view synthesis in outdoor dynamic scenes. RF4D explicitly incorporates temporal information into its representation, significantly enhancing its capability to model moving objects. We further introduce a feature-level flow module that predicts latent temporal offsets between adjacent frames, enforcing temporal coherence in dynamic scene modeling. Moreover, we propose a radar-specific power rendering formulation closely aligned with radar sensing physics, improving synthesis accuracy and interoperability. Extensive experiments on public radar datasets demonstrate the superior performance of RF4D in terms of radar measurement synthesis quality and occupancy estimation accuracy, achieving especially pronounced improvements in dynamic outdoor scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  
<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="80%">
        <!-- Your video here -->
        <source src="static/videos/demo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Our RF4D is able to reconstruct the 3D geometry of a scene from radar measurements. LiDAR point clouds are also provided for reference purposes.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<section class="section">
  <div class="container is-max-desktop">
  
      <div class="columns is-centered">
      <div class="column is-full-width">
          <h2 class="title is-3">Method overview</h2>
          <img src="static/images/fig2.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>

          <div class="content has-text-justified">
            <p>
               Overview of the proposed RF4D framework. Given a 3D query point $(x,y,z)$ at time $t$ and view direction $d$, RF4D first predicts two radar-specific physical quantities: occupancy $\alpha$ and radar cross section (RCS) $\sigma$, using neural radar fields. The predicted occupancy reflects whether the point is physically occupied, while the RCS describes its reflectivity. Then these quantities are combined via our proposed radar-specific power rendering to estimate the received radar power. During training, besides the supervision of ground truth radar measurements, the feature-level flow module promotes temporal consistency of occupancy by modeling latent feature changes across adjacent frames.
              </p>
          </div>
      </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
  
      <div class="columns is-centered">
      <div class="column is-full-width">
          <h2 class="title is-3">Qualitative comparison</h2>
          <img src="static/images/fig4_2.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>

          <div class="content has-text-justified">
            <p>
              Qualitative comparison of novel-view radar measurement synthesis (top) and occupancy estimation (bottom) across different methods on the RobotCar dataset. Ground truth occupancy is derived from LiDAR point clouds. Our method accurately reconstructs the radar measurements with clear structure and minimal artifacts, while RadarFields introduces noise and blurring, especially around dynamic objects. D-NeRF and DyNeRF, which rely on volume rendering, fail to recover meaningful scene structure. In contrast, our predictions are clean and closely matching the ground truth.
              </p>
          </div>
      </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
  
      <div class="columns is-centered">
      <div class="column is-full-width">
          <h2 class="title is-3"> 3D voxel grid reconstruction</h2>
          <img src="static/images/appendix_3d.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>

          <div class="content has-text-justified">
            <p>
              Our method reconstructs full 3D occupancy geometry from sparse and low-resolution radar data, capturing both moving vehicles and static objects present in the scene. LiDAR point clouds axsnd scene images are provided for reference only.
              </p>
          </div>
      </div>
      </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{zhang2025rf4d,
  title={RF4D: Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes},
  author={Zhang, Jiarui and Li, Zhihao and Wang, Chong and Wen, Bihan},
  journal={arXiv preprint arXiv:2505.20967},
  year={2025}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
